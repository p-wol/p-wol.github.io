<!doctype html>
<html lang="en">
    <head>
        <meta name="description" content="Homepage of Pierre Wolinski">
        <meta name="keywords" content="Pierre,Wolinski,homepage">
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<link rel="stylesheet" type="text/css" title="CSSPierreWolinski" href="main.css">
	<title>Homepage of Pierre Wolinski</title>
    </head>
    <body>
        <h1>Pierre Wolinski</h1>
	<div class="divphoto">
	<img class="imgphoto" src="Photo2.JPG" alt="Photo Pierre Wolinski">
	</div>
	<p>I am currently a post-doctoral researcher in the <a href="https://team.inria.fr/statify/">Statify team</a>, Inria Grenoble-Alpes (France), after one year at the Department of Statistics of the University of Oxford. My supervisors are Julyan Arbel and Judith Rousseau.</p>
	<p>I completed my PhD in Machine Learning in March 2019, supervised by Guillaume Charpiat and Yann Ollivier, at the TAU team, LRI.<br/>
	<a href="CV_eng.pdf">Here is my CV</a>.
	</p>
	<p>Thesis: <em>Structural Learning of Neural Networks</em>
	[<a href="These.pdf">final version</a>].<br/>
	Defense carried out on March 6th, 2020.
	<h2>Interest</h2>
	<ul>
		<li>theory: Bayesian/Variational inference, kernel methods;</li>
		<li>AutoML: neural network pruning, neural architecture search;</li>
		<li>other: neural tangent kernels, random weights (Lottery Ticket Hypothesis, Extreme Learning Machines...).</li>
	</ul>
	<h2>Publications</h2>
	<ul>
		<li><em>Rethinking Gauss-Newton for learning over-parameterized models</em> (2023), 
			M. Arbel, R. Ménégaux, P. Wolinski
			[<a href="https://arxiv.org/abs/2302.02904">link</a>]</li>
		<li><em>Gaussian Pre-Activations in Neural Networks: Myth or Reality?</em> (2022), 
			P. Wolinski, J. Arbel
			[<a href="https://arxiv.org/abs/2205.12379">link</a>]</li>
		<li><em>Interpreting a Penalty as the Influence of a Bayesian Prior</em> (2020), 
			P. Wolinski, G. Charpiat, Y. Ollivier
			[<a href="https://arxiv.org/abs/2002.00178">link</a>]</li>
		<li><em>Learning with Random Learning Rates</em> (ECML 2019), L. Blier, P. Wolinski, Y. Ollivier 
			[<a href="https://arxiv.org/abs/1810.01322">link</a>]
			[<a href="https://github.com/leonardblier/alrao/">code</a>]</li>
		<li><em>Asymmetrical Scaling Layers for Stable Network Pruning</em>, 
			P. Wolinski, G. Charpiat, Y. Ollivier 
			[<a href="Wolinski2019_Asymmetrical_Scaling_for_Stable_Network_Pruning.pdf">draft</a>]</li>
		<li><em>Consistance des méthodes RKHS dans le cadre de la minimisation d’un risque convexe</em> (Master thesis, 2015),
			P. Wolinski, supervised by F. d'Alché-Buc, E. Moulines, F. Roueff [link]</li>
	</ul>
	<h2>Curriculum Vitae</h2>
	<ul>
		<li>PhD in Computer Science, University Paris-Saclay (formerly University Paris-Sud).</li>
		<li>Graduate degree in Mathematics (Physics option) at the École Normale Supérieure (Paris).</li>
		<li>Master degree in Mathematics (Probability and Statistics), University Paris-Sud.</li>
	</ul>
	<h3>Study</h3>
	<p>2021 - now: post-doctoral researcher in the Statify team, Inria Grenoble-Alpes (France).</p>
	<p>2020 - 2021: post-doctoral researcher at the Department of Statistics of the University of Oxford.</p>
	<p>2016 - 2020: PhD student in Computer Science at the team TAU, LRI, University Paris-Sud/Inria, France.</p>
	<p>2011 - 2016: Math student at the École Normale Supérieure, Paris, France.</p>
	<p>2008 - 2011: Physics/Chemistry student in "classe préparatoire".</p>
	<p>2008: Baccalauréat.</p>
    </body>
</html>


