<!doctype html>
<html lang="en">
    <head>
        <meta name="description" content="Homepage of Pierre Wolinski">
        <meta name="keywords" content="Pierre,Wolinski,homepage">
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<link rel="stylesheet" type="text/css" title="CSSPierreWolinski" href="main.css">
	<title>Homepage of Pierre Wolinski</title>
    </head>
    <body>
        <h1>Pierre Wolinski</h1>
	<div class="divphoto">
	<img class="imgphoto" src="Photo2.JPG" alt="Photo Pierre Wolinski">
	</div>
	<p>Since September 2024, I have been an assistant professor (ma&icirc;tre de conférences) in Computer Science, in the Data Science team, 
		at the <a href="https://www.lamsade.dauphine.fr/">LAMSADE</a>, Paris Dauphine University &ndash; PSL, France.
	<p>I completed my PhD in Machine Learning in March 2020, supervised by Guillaume Charpiat and Yann Ollivier, at the TAU team, LRI.<br/>
	Here is my CV: [<a href="CV_2024_06_eng.pdf">eng</a>].
	</p>
	<p>Thesis: <em>Structural Learning of Neural Networks</em>
	[<a href="These.pdf">PDF file</a>] 
	[<a href="https://theses.fr/2020UPASS026">link</a>].<br/>
	Defense carried out on March 6th, 2020.
	<h2>Interest</h2>
	<ul>
		<li>deep learning, neural networks (NNs);</li>
		<li>theory: optimization, Bayesian/Variational inference;</li>
		<li>hyperparameter search, tuning or removal: neural network pruning, learning rate tuning;</li>
		<li>other: neural tangent kernels, random weights, weight initialization...</li>
	</ul>
	<h2>Works</h2>
	<ul>
		<li><em>Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives</em> (2023), 
			P. Wolinski
			[<a href="https://arxiv.org/abs/2312.03885">link</a>]
			[<a href="https://github.com/p-wol/GroupedNewton">code</a>]</li>
		<li><em>Efficient Neural Networks for Tiny Machine Learning: A Comprehensive Review</em> (2023), 
			M. T. L&ecirc;, P. Wolinski, J. Arbel
			[<a href="https://arxiv.org/abs/2311.11883">link</a>]</li>
		<li><em>Rethinking Gauss-Newton for learning over-parameterized models</em> (NeurIPS 2023, poster), 
			M. Arbel, R. Ménégaux, P. Wolinski
			[<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/6a14c7f9fb3f42645cfa6bd5aa446819-Paper-Conference.pdf">link</a>]</li>
		<li><em>Gaussian Pre-Activations in Neural Networks: Myth or Reality?</em> (2022), 
			P. Wolinski, J. Arbel
			[<a href="https://arxiv.org/abs/2205.12379">link</a>]
			[<a href="https://github.com/p-wol/gaussian-preact">code</a>]</li>
		<li><em>An Equivalence between Bayesian Priors and Penalties in Variational Inference</em> (2020), 
			P. Wolinski, G. Charpiat, Y. Ollivier
			[<a href="https://arxiv.org/abs/2002.00178">link</a>]</li>
		<li><em>Learning with Random Learning Rates</em> (ECML 2019), L. Blier, P. Wolinski, Y. Ollivier 
			[<a href="https://arxiv.org/abs/1810.01322">link</a>]
			[<a href="https://github.com/leonardblier/alrao/">code</a>]</li>
		<li><em>Asymmetrical Scaling Layers for Stable Network Pruning</em>, 
			P. Wolinski, G. Charpiat, Y. Ollivier 
			[<a href="Wolinski2019_Asymmetrical_Scaling_for_Stable_Network_Pruning.pdf">draft</a>]</li>
		<li><em>Consistance des méthodes RKHS dans le cadre de la minimisation d’un risque convexe</em> (Master thesis, 2015),
			P. Wolinski, supervised by F. d'Alché-Buc, E. Moulines, F. Roueff [link]</li>
	</ul>
	<h2>Curriculum Vitae</h2>
	<h3>Career</h3>
	<p>2024 &ndash; now: assistant professor (ma&icirc;tre de conférences) in Computer Science, <a href="https://www.lamsade.dauphine.fr/">LAMSADE</a>, Paris Dauphine University &ndash; PSL, France.</p>
	<p>2023 &ndash; 2024: post-doctoral researcher in the <a href="https://www.imo.universite-paris-saclay.fr/en/research/probability-and-statistics/">Probability and Statistics team</a>, 
		<a href="https://www.universite-paris-saclay.fr/">Paris-Saclay University</a>, France (supervised by 
		<a href="https://www.imo.universite-paris-saclay.fr/~gilles.blanchard/">Gilles Blanchard</a> and 
		<a href="https://www.imo.universite-paris-saclay.fr/~christophe.giraud/">Christophe Giraud</a>).</p>
	<p>2021 &ndash; 2023: post-doctoral researcher in the <a href="https://team.inria.fr/statify/">Statify team</a>, 
		<a href="https://www.inria.fr/en/inria-centre-university-grenoble-alpes">Inria Grenoble-Alpes</a>, France 
		(supervised by <a href="https://www.julyanarbel.com/">Julyan Arbel</a>).</p>
	<p>2020 &ndash; 2021: post-doctoral researcher at the <a href="https://www.stats.ox.ac.uk/">Department of Statistics</a>, 
		<a href="https://www.ox.ac.uk/">University of Oxford</a>, UK 
		(supervised by <a href="https://dauphine.psl.eu/en/research/resume-database/rousseau-judith">Judith Rousseau</a>).</p>
	<h3>Degrees</h3>
	<p>2020: PhD in Computer Science, <a href="https://www.universite-paris-saclay.fr/">Paris-Saclay University</a>.</p>
	<p>2017: Graduate degree in Mathematics (minor in Physics) at the <a href="https://www.ens.psl.eu/">École Normale Supérieure (Paris)</a>.</p>
	<p>2016: Master degree in Mathematics (Probability and Statistics), <a href="https://www.universite-paris-saclay.fr/">Paris-Sud University</a>.</p>
	<h3>Study</h3>
	<p>2016 &ndash; 2020: PhD student in Computer Science at the <a href="https://www.inria.fr/fr/tau">TAO/Tau team</a>, LRI, 
		<a href="https://www.universite-paris-saclay.fr/">Paris-Sud University</a>/<a href="https://inria.fr/en/inria-saclay-centre">Inria Saclay</a>, 
		France (supervised by <a href="https://www.lri.fr/~gcharpia/index_en.html">Guillaume Charpiat</a> 
		and <a href="http://www.yann-ollivier.org/">Yann Ollivier</a>).</p>
	<p>2011 &ndash; 2016: Math student at the <a href="https://www.ens.psl.eu/">École Normale Supérieure</a>, Paris, France.</p>
	<p>2008 &ndash; 2011: Physics/Chemistry student in "<a href="https://pia.ac-paris.fr/serail/jcms/s2_1470075/fr/accueil">classe préparatoire</a>".</p>
	<h2>Various links</h2>
	<p><a href="https://orcid.org/0000-0003-1007-0144">ORCID</a>,
	   <a href="https://github.com/p-wol">GitHub</a></p>
    </body>
</html>


