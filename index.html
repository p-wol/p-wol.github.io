<!doctype html>
<html lang="en">
    <head>
        <meta name="description" content="Homepage of Pierre Wolinski">
        <meta name="keywords" content="Pierre,Wolinski,homepage">
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<link rel="stylesheet" type="text/css" title="CSSPierreWolinski" href="main.css">
	<title>Homepage of Pierre Wolinski</title>
    </head>
    <body>
        <h1>Pierre Wolinski</h1>
	<div class="divphoto">
	<img class="imgphoto" src="Photo2.JPG" alt="Photo Pierre Wolinski">
	</div>
	<p>Since October 2023, I am a post-doctoral researcher in the <a href="https://www.imo.universite-paris-saclay.fr/en/research/probability-and-statistics/">Probability and Statistics team</a>, 
		Institut de Mathématiques d'Orsay, Université Paris-Saclay (France). 
		My supervisors are Gilles Blanchard and Christophe Giraud.</p>
	<p>I completed my PhD in Machine Learning in March 2020, supervised by Guillaume Charpiat and Yann Ollivier, at the TAU team, LRI.<br/>
	Here is my CV: [<a href="CV_2024_06_eng.pdf">eng</a>].
	</p>
	<p>Thesis: <em>Structural Learning of Neural Networks</em>
	[<a href="These.pdf">final version</a>].<br/>
	Defense carried out on March 6th, 2020.
	<h2>Interest</h2>
	<ul>
		<li>deep learning, neural networks (NNs);</li>
		<li>theory: optimization, Bayesian/Variational inference;</li>
		<li>hyperparameter search, tuning or removal: neural network pruning, learning rate tuning;</li>
		<li>other: neural tangent kernels, random weights, weight initialization...</li>
	</ul>
	<h2>Works</h2>
	<ul>
		<li><em>Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives</em> (2023), 
			P. Wolinski
			[<a href="https://arxiv.org/abs/2312.03885">link</a>]
			[<a href="https://github.com/p-wol/GroupedNewton">code</a>]</li>
		<li><em>Efficient Neural Networks for Tiny Machine Learning: A Comprehensive Review</em> (2023), 
			M. T. L&ecirc;, P. Wolinski, J. Arbel
			[<a href="https://arxiv.org/abs/2311.11883">link</a>]</li>
		<li><em>Rethinking Gauss-Newton for learning over-parameterized models</em> (NeurIPS 2023, poster), 
			M. Arbel, R. Ménégaux, P. Wolinski
			[<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/6a14c7f9fb3f42645cfa6bd5aa446819-Paper-Conference.pdf">link</a>]</li>
		<li><em>Gaussian Pre-Activations in Neural Networks: Myth or Reality?</em> (2022), 
			P. Wolinski, J. Arbel
			[<a href="https://arxiv.org/abs/2205.12379">link</a>]
			[<a href="https://github.com/p-wol/gaussian-preact">code</a>]</li>
		<li><em>An Equivalence between Bayesian Priors and Penalties in Variational Inference</em> (2020), 
			P. Wolinski, G. Charpiat, Y. Ollivier
			[<a href="https://arxiv.org/abs/2002.00178">link</a>]</li>
		<li><em>Learning with Random Learning Rates</em> (ECML 2019), L. Blier, P. Wolinski, Y. Ollivier 
			[<a href="https://arxiv.org/abs/1810.01322">link</a>]
			[<a href="https://github.com/leonardblier/alrao/">code</a>]</li>
		<li><em>Asymmetrical Scaling Layers for Stable Network Pruning</em>, 
			P. Wolinski, G. Charpiat, Y. Ollivier 
			[<a href="Wolinski2019_Asymmetrical_Scaling_for_Stable_Network_Pruning.pdf">draft</a>]</li>
		<li><em>Consistance des méthodes RKHS dans le cadre de la minimisation d’un risque convexe</em> (Master thesis, 2015),
			P. Wolinski, supervised by F. d'Alché-Buc, E. Moulines, F. Roueff [link]</li>
	</ul>
	<h2>Curriculum Vitae</h2>
	<h3>Degrees</h3>
	<ul>
		<li>2020: PhD in Computer Science, University Paris-Saclay (formerly University Paris-Sud).</li>
		<li>2017: Graduate degree in Mathematics (Physics option) at the École Normale Supérieure (Paris).</li>
		<li>2016: Master degree in Mathematics (Probability and Statistics), University Paris-Sud.</li>
	</ul>
	<h3>Professional experience</h3>
	<p>2023 - now: post-doctoral researcher in the Probability and Statistics team, Université Paris-Saclay, France (supervised by Gilles Blanchard and Christophe Giraud).</p>
	<p>2021 - 2023: post-doctoral researcher in the Statify team, Inria Grenoble-Alpes, France (supervised by Julyan Arbel).</p>
	<p>2020 - 2021: post-doctoral researcher at the Department of Statistics of the University of Oxford, UK (supervised by Judith Rousseau).</p>
	<h3>Study</h3>
	<p>2016 - 2020: PhD student in Computer Science at the team TAU, LRI, University Paris-Sud/Inria, France (supervised by Guillaume Charpiat and Yann Ollivier).</p>
	<p>2011 - 2016: Math student at the École Normale Supérieure, Paris, France.</p>
	<p>2008 - 2011: Physics/Chemistry student in "classe préparatoire".</p>
	<p>2008: Baccalauréat.</p>
    </body>
</html>


